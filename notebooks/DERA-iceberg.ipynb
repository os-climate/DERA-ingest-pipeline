{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abf0869-f139-411a-9b34-51945c12cade",
   "metadata": {},
   "source": [
    "# Ingest SEC DERA data into Trino pipeline\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "This sample shows:\n",
    "* How to create schemas and tables via the Trino / SQLAlchemy on an underlying Iceberg data volume\n",
    "* Apache Iceberg ACID transaction and time travel capabilities used for data set versioning\n",
    "\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feeb9fa-dbfd-44d9-a2c5-e54983b48ead",
   "metadata": {},
   "source": [
    "Run these in a notebook cell if you need to install onto your nb env\n",
    "\n",
    "```python\n",
    "# 'capture' magic prevents long outputs from spamming your notebook\n",
    "%%capture pipoutput\n",
    "\n",
    "# For loading predefined environment variables from files\n",
    "# Typically used to load sensitive access credentials\n",
    "%pip install python-dotenv\n",
    "\n",
    "# Standard python package for interacting with S3 buckets\n",
    "%pip install boto3\n",
    "\n",
    "# Interacting with Trino and using Trino with sqlalchemy\n",
    "%pip install trino sqlalchemy sqlalchemy-trino\n",
    "\n",
    "# Pandas and parquet file i/o\n",
    "%pip install pandas pyarrow fastparquet\n",
    "\n",
    "# OS-Climate utilities to make data ingest easier\n",
    "%pip install osc-ingest-tools\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7692120-2f4f-48be-9847-0f78a3359bc1",
   "metadata": {},
   "source": [
    "Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d833a-07f7-42b6-bbbc-a78ab919f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "dotenv_dir = os.environ.get(\"CREDENTIAL_DOTENV_DIR\", os.environ.get(\"PWD\", \"/opt/app-root/src\"))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / \"credentials.env\"\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fa84b-d4c0-4dd6-99da-2cab1641ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_source = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ[\"S3_LANDING_ENDPOINT\"],\n",
    "    aws_access_key_id=os.environ[\"S3_LANDING_ACCESS_KEY\"],\n",
    "    aws_secret_access_key=os.environ[\"S3_LANDING_SECRET_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195f360-a597-496c-b2f9-f9463d69cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_catalog = \"osc_datacommons_iceberg_dev\"\n",
    "ingest_schema = \"sec_dera\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97e0d8-a088-459e-8e8d-98105b05a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# telling sqlalchemy about catalog has to be done in the sqlstring url:\n",
    "import trino\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "sqlstring = \"trino://{user}@{host}:{port}/{catalog}\".format(\n",
    "    user=os.environ[\"TRINO_USER\"], host=os.environ[\"TRINO_HOST\"], port=os.environ[\"TRINO_PORT\"], catalog=ingest_catalog\n",
    ")\n",
    "sqlargs = {\"auth\": trino.auth.JWTAuthentication(os.environ[\"TRINO_PASSWD\"]), \"http_scheme\": \"https\"}\n",
    "engine = create_engine(sqlstring, connect_args=sqlargs)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94102c-8c8e-450a-b95e-ea5a368fe09f",
   "metadata": {},
   "source": [
    "Drop previous tables and schema to start with a fresh slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f43716-2eaa-4c46-941b-41399032540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ingest_table in [\"sub\", \"num\", \"tag\"]:\n",
    "    sql = f\"\"\"\n",
    "drop table if exists {ingest_catalog}.{ingest_schema}.{ingest_table}\n",
    "\"\"\"\n",
    "    print(sql)\n",
    "    qres = engine.execute(sql)\n",
    "    print(qres.fetchall())\n",
    "\n",
    "sql = f\"\"\"\n",
    "drop schema if exists {ingest_catalog}.{ingest_schema}\n",
    "\"\"\"\n",
    "print(sql)\n",
    "qres = engine.execute(sql)\n",
    "print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a9ec9-1ae4-49b5-8ec7-12839341466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure schema exists, or table creation below will fail in weird ways\n",
    "sql = f\"\"\"\n",
    "create schema {ingest_catalog}.{ingest_schema}\n",
    "\"\"\"\n",
    "print(sql)\n",
    "qres = engine.execute(sql)\n",
    "print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16125762-952a-4585-b98c-f22afec4fec8",
   "metadata": {},
   "source": [
    "Enter the Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f826ef9c-87d5-45fc-b624-af1d44b6e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc764ad6-0862-4f0d-a843-c6ba978c4ced",
   "metadata": {},
   "source": [
    "Prepare GLEIF matching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c153c48-a2d9-4009-bf94-2968377731c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleif_file = s3_source.Object(os.environ[\"S3_LANDING_BUCKET\"], f\"mtiemann-GLEIF/DERA-matches.csv\")\n",
    "gleif_file.download_file(f\"/tmp/dera-gleif.csv\")\n",
    "gleif_df = pd.read_csv(f\"/tmp/dera-gleif.csv\", header=0, sep=\",\", dtype=str, engine=\"c\")\n",
    "gleif_dict = {k: v for k, v in zip(gleif_df.name, gleif_df.LEI)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28934131-b6c6-496d-8459-c8880bc5a15a",
   "metadata": {},
   "source": [
    "Load the SUB and NUM tables into Trino.  READS ONLY 10 ROWS RIGHT NOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa805c-42f3-4396-bc95-c5a11c6a711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import osc_ingest_trino as osc\n",
    "\n",
    "_p2smap = {\n",
    "    \"string\": \"varchar\",\n",
    "    \"float32\": \"real\",\n",
    "    \"Float32\": \"real\",\n",
    "    \"float64\": \"double\",\n",
    "    \"Float64\": \"double\",\n",
    "    \"int32\": \"integer\",\n",
    "    \"Int32\": \"integer\",\n",
    "    \"int64\": \"bigint\",\n",
    "    \"Int64\": \"bigint\",\n",
    "    \"bool\": \"boolean\",\n",
    "    \"category\": \"varchar\",\n",
    "    \"datetime64[ns, UTC]\": \"timestamp(6)\",\n",
    "}\n",
    "\n",
    "\n",
    "def pandas_type_to_sql(pt):\n",
    "    st = _p2smap.get(pt)\n",
    "    if st is not None:\n",
    "        return st\n",
    "    raise ValueError(\"unexpected pandas column type '{pt}'\".format(pt=pt))\n",
    "\n",
    "\n",
    "# add ability to specify optional dict for specific fields?\n",
    "# if column name is present, use specified value?\n",
    "def create_table_schema_pairs(df):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"df must be a pandas DataFrame\")\n",
    "    ptypes = [str(e) for e in df.dtypes.to_list()]\n",
    "    stypes = [pandas_type_to_sql(e) for e in ptypes]\n",
    "    pz = list(zip(df.columns.to_list(), stypes))\n",
    "    return \",\\n\".join([\"    {n} {t}\".format(n=e[0], t=e[1]) for e in pz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54f993-928f-464d-87bf-21e920ea851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import uuid\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# Add a unique identifier to the data set\n",
    "uid = str(uuid.uuid4())\n",
    "\n",
    "dera_regex = re.compile(r\" ?/.*$\")\n",
    "\n",
    "quarters = [\"2020q1\", \"2020q2\", \"2020q3\", \"2020q4\", \"2021q1\", \"2021q2\", \"2021q3\"]\n",
    "\n",
    "# If our dataframe has more than this many rows, we need to chunk.\n",
    "# ??? Should use df.info to better calculate the appropriate max_rowcount\n",
    "max_rowcount = 78\n",
    "\n",
    "last_df = None\n",
    "\n",
    "\n",
    "def ingest_dera_table(qtr, tbl):\n",
    "    global last_df\n",
    "\n",
    "    src_file = s3_source.Object(os.environ[\"S3_LANDING_BUCKET\"], f\"SEC-DERA/{qtr}/{tbl}.txt\")\n",
    "    timestamp = src_file.last_modified.isoformat()\n",
    "    src_file.download_file(f\"/tmp/dera-{tbl}-{timestamp}.csv\")\n",
    "    df = pd.read_csv(\n",
    "        f\"/tmp/dera-{tbl}-{timestamp}.csv\",\n",
    "        header=0,\n",
    "        sep=\"\\t\",\n",
    "        dtype=\"string\",\n",
    "        keep_default_na=False,\n",
    "        nrows=None,\n",
    "        engine=\"c\",\n",
    "    )\n",
    "\n",
    "    df[\"uuid\"] = uid\n",
    "    df[\"quarter\"] = qtr\n",
    "    df = df.convert_dtypes(\n",
    "        infer_objects=False, convert_string=True, convert_integer=False, convert_boolean=False, convert_floating=False\n",
    "    )\n",
    "    # Print the output\n",
    "    # print(df.dtypes)\n",
    "\n",
    "    if tbl == \"sub\":\n",
    "        df.name = df.name.map(lambda x: re.sub(dera_regex, \"\", x))\n",
    "        df.name = df.name.astype(\"string\")\n",
    "        df[\"LEI\"] = df.name.map(gleif_dict)\n",
    "        df.LEI = df.LEI.astype(\"string\")\n",
    "        df.cik = df.cik.astype(\"int32\")\n",
    "        df.loc[df.sic == \"\", \"sic\"] = pd.NA\n",
    "        df.sic = df.sic.astype(\"Int32\")\n",
    "        df.loc[df.ein == \"\", \"ein\"] = pd.NA\n",
    "        df.ein = df.ein.astype(\"Int64\")\n",
    "        df.wksi = df.wksi.astype(\"bool\")\n",
    "        # df.wksi = df.wksi.astype('int32')\n",
    "        df.period = pd.to_datetime(df.period, format=\"%Y%m%d\", utc=True, errors=\"coerce\")\n",
    "        df.fy = pd.to_datetime(df.fy, format=\"%Y\", utc=True, errors=\"coerce\")\n",
    "        df.filed = pd.to_datetime(df.filed, format=\"%Y%m%d\", utc=True)\n",
    "        df.accepted = pd.to_datetime(df.accepted, format=\"%Y-%m-%d %H:%M:%S\", utc=True)\n",
    "        df.prevrpt = df.prevrpt.astype(\"bool\")\n",
    "        df.detail = df.detail.astype(\"bool\")\n",
    "        # df.prevrpt = df.prevrpt.astype('int32')\n",
    "        # df.detail = df.detail.astype('int32')\n",
    "        df.nciks = df.nciks.astype(\"int32\")\n",
    "\n",
    "        cols = df.columns.tolist()\n",
    "        cols = cols[0:3] + [cols[-1]] + cols[3:-1]\n",
    "        df = df[cols]\n",
    "    elif tbl == \"num\":\n",
    "        # documentation wrongly lists coreg as NUMERIC length 256.  It is ALPHANUMERIC.\n",
    "        df.ddate = pd.to_datetime(df.ddate, format=\"%Y%m%d\", utc=True)\n",
    "        df.qtrs = df.qtrs.astype(\"int32\")\n",
    "        df.value = df.value.astype(\"float64\")\n",
    "    elif tbl == \"tag\":\n",
    "        df.custom = df.custom.astype(\"bool\")\n",
    "        df.abstract = df.abstract.astype(\"bool\")\n",
    "    print(df.dtypes)\n",
    "    display(df.head())\n",
    "\n",
    "    # Only drop table and create new one if we are starting from the beginning\n",
    "    if qtr == quarters[0]:\n",
    "        table_check = engine.execute(f\"drop table if exists {ingest_catalog}.{ingest_schema}.{tbl}\")\n",
    "        for row in table_check.fetchall():\n",
    "            print(row)\n",
    "\n",
    "        columnschema = create_table_schema_pairs(df)\n",
    "\n",
    "        tabledef = f\"\"\"\n",
    "create table if not exists {ingest_catalog}.{ingest_schema}.{tbl}(\n",
    "{columnschema}\n",
    ") with (\n",
    "    format = 'ORC',\n",
    "    partitioning = array['quarter']\n",
    ")\n",
    "\"\"\"\n",
    "        print(tabledef)\n",
    "        qres = engine.execute(tabledef)\n",
    "        print(qres.fetchall())\n",
    "\n",
    "    # method = 'multi' is important, default will not work\n",
    "    # important to tell it about schema here, and catalog when you create the db connection above\n",
    "    # index = False, unless you declared that as a column when you create the table\n",
    "    # use 'append' mode since we already created the table\n",
    "    last_df = df\n",
    "    time1 = datetime.now()\n",
    "    for i in range(0, len(df.index), max_rowcount):\n",
    "        row_max = min(max_rowcount, len(df.index))\n",
    "        print(f\"{str(datetime.now())}: i = {i}; row_max = {row_max}, len(df.index) = {len(df.index)}\")\n",
    "        df.iloc[i : i + row_max].to_sql(\n",
    "            tbl, con=engine, schema=ingest_schema, if_exists=\"append\", index=False, method=\"multi\"\n",
    "        )\n",
    "        time2 = datetime.now()\n",
    "        tdiff = time2 - time1\n",
    "        print(f\"tiff = {tdiff}; ETA = {tdiff * len(df.index) / (i+row_max)}\")\n",
    "\n",
    "    sql = f\"\"\"\n",
    "select * from {ingest_catalog}.{ingest_schema}.{tbl} limit 10\n",
    "\"\"\"\n",
    "    pd.read_sql(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e41537-3a0e-4b14-8a4b-4ca7bc79027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_tests = False\n",
    "\n",
    "for qtr in quarters:\n",
    "    for ingest_table in [\"sub\", \"num\", \"tag\"]:\n",
    "        print(f\"Ingesting table {ingest_table}; quarter = {qtr}; timestamp = {str(datetime.now())}\")\n",
    "\n",
    "        ingest_dera_table(qtr, ingest_table)\n",
    "\n",
    "        if not snapshot_tests:\n",
    "            continue\n",
    "\n",
    "        sql = f\"\"\"\n",
    "select snapshot_id, committed_at from {ingest_catalog}.{ingest_schema}.\"{ingest_table}$snapshots\"\n",
    "    order by committed_at desc\n",
    "    limit 10\n",
    "\"\"\"\n",
    "        print(sql)\n",
    "        qres = engine.execute(sql)\n",
    "        snapshots = qres.fetchall()\n",
    "        display(snapshots)\n",
    "\n",
    "        previous_snapshot = snapshots[1][0]\n",
    "        previous_snapshot\n",
    "\n",
    "        sql = f\"\"\"\n",
    "call {ingest_catalog}.system.rollback_to_snapshot('{ingest_schema}', '{ingest_table}', {previous_snapshot})\n",
    "\"\"\"\n",
    "        print(sql)\n",
    "        qres = engine.execute(sql)\n",
    "        print(qres.fetchall())\n",
    "\n",
    "        sql = f\"\"\"\n",
    "select * from {ingest_catalog}.{ingest_schema}.{ingest_table}\n",
    "\"\"\"\n",
    "        pd.read_sql(sql, engine)\n",
    "\n",
    "        sql = f\"\"\"\n",
    "call {ingest_catalog}.system.rollback_to_snapshot('{ingest_schema}', '{ingest_table}', {snapshots[0][0]})\n",
    "\"\"\"\n",
    "        print(sql)\n",
    "        qres = engine.execute(sql)\n",
    "        print(qres.fetchall())\n",
    "\n",
    "        sql = f\"\"\"\n",
    "select snapshot_id, committed_at from {ingest_catalog}.{ingest_schema}.\"{ingest_table}$snapshots\"\n",
    "    order by committed_at desc\n",
    "    limit 10\n",
    "\"\"\"\n",
    "        qres = engine.execute(sql)\n",
    "        snapshots = qres.fetchall()\n",
    "        display(snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9bcb1-367e-4301-bc21-d28d742f6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7468eb8b-68e0-4e5b-850f-46102fabe1cd",
   "metadata": {},
   "source": [
    "ingest_table = 'sub'\n",
    "\n",
    "sql=f\"\"\"\n",
    "select snapshot_id, committed_at from {ingest_catalog}.{ingest_schema}.\"{ingest_table}$snapshots\"\n",
    "    order by committed_at desc\n",
    "    limit 10\n",
    "\"\"\"\n",
    "print(sql)\n",
    "qres = engine.execute(sql)\n",
    "snapshots = qres.fetchall()\n",
    "snapshots"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61f0bd50-71fc-4bc2-9b3f-dfde39cf14d5",
   "metadata": {},
   "source": [
    "previous_snapshot = snapshots[1][0]\n",
    "previous_snapshot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdbf2cd6-87f5-4745-b8dc-4453ac505949",
   "metadata": {},
   "source": [
    "sql=f\"\"\"\n",
    "call {ingest_catalog}.system.rollback_to_snapshot('{ingest_schema}', '{ingest_table}', {previous_snapshot})\n",
    "\"\"\"\n",
    "print(sql)\n",
    "qres = engine.execute(sql)\n",
    "print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89c666d6-e2a9-4df0-b2da-4b7a03162b9c",
   "metadata": {},
   "source": [
    "sql=f\"\"\"\n",
    "select * from {ingest_catalog}.{ingest_schema}.{ingest_table}\n",
    "\"\"\"\n",
    "pd.read_sql(sql, engine)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b444b69c-19cd-4297-a762-7552433bb3e4",
   "metadata": {},
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f87b7dd-d925-4eac-a5d4-edb831d3756f",
   "metadata": {},
   "source": [
    "Create metadata table for schema / dataset level information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2dc67-842b-4b9e-98ed-64e1261b2c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variable names for metadata structure in Trino\n",
    "meta_schema_name = \"metastore_iceberg\"\n",
    "meta_table_name_dataset = \"meta_tables_iceberg\"\n",
    "meta_table_name_fields = \"meta_fields_iceberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfeed99-2cf2-48d5-bda8-fef262858e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_meta_content = {\n",
    "    \"dataset_key\": \"SEC-DERA\",\n",
    "    \"tablename\": \"sub\",\n",
    "    \"title\": \"SEC DERA Disclosures: SUB\",\n",
    "    \"description\": \"\"\"The DERA Financial Statement Data Sets provide numeric information from the face financials of all financial statements.\n",
    "    \n",
    "    This data is extracted from exhibits to corporate financial reports filed with the Commission using eXtensible Business Reporting Language (XBRL).  As compared to the more extensive Financial Statement and Notes Data Sets, which provide the numeric and narrative disclosures from all financial statements and their notes, the Financial Statement Data Sets are more compact.\"\"\",\n",
    "    \"version\": \"quarterly\",\n",
    "    \"release_date\": \"rolling\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"adsh\": \"Accession Number. The 20-character string formed from the 18-digit number assigned by the SEC to each EDGAR submission.\",\n",
    "            \"cik\": \"Central Index Key (CIK). Ten digit number assigned by the SEC to each registrant that submits filings.\",\n",
    "            \"name\": \"Name of registrant. This corresponds to the name of the legal entity as recorded in EDGAR as of the filing date.\",\n",
    "            \"sic\": \"Standard Industrial Classification (SIC). Four digit code assigned by the SEC as of the filing date, indicating the registrant’s type of business.\",\n",
    "            \"countryba\": \"The ISO 3166-1 country of the registrant’s business address.\",\n",
    "            \"stprba\": \"The state or province of the registrant’s business address, if field countryba is US or CA.\",\n",
    "            \"cityba\": \"The city of the registrant’s business address.\",\n",
    "            \"zipba\": \"The zip code of the registrant’s business address.\",\n",
    "            \"bas1\": \"The first line of the street of the registrant’s business address.\",\n",
    "            \"bas2\": \"The second line of the street of the registrant’s business address.\",\n",
    "            \"baph\": \"The phone number of the registrant’s business address.\",\n",
    "            \"countryma\": \"The ISO 3166-1 country of the registrant’s mailing address.\",\n",
    "            \"stprma\": \"The state or province of the registrant’s mailing address, if field countryma is US or CA.\",\n",
    "            \"cityma\": \"The city of the registrant’s mailing address.\",\n",
    "            \"zipma\": \"The zip code of the registrant’s mailing address.\",\n",
    "            \"mas1\": \"The first line of the street of the registrant’s mailing address.\",\n",
    "            \"mas2\": \"The second line of the street of the registrant’s mailing address.\",\n",
    "            \"countryinc\": \"The country of incorporation for the registrant.\",\n",
    "            \"stprinc\": \"The state or province of incorporation for the registrant, if countryinc is US or CA.\",\n",
    "            \"ein\": \"Employee Identification Number, 9 digit identification number assigned by the Internal Revenue Service to business entities operating in the United States.\",\n",
    "            \"former\": \"Most recent former name of the registrant, if any.\",\n",
    "            \"changed\": \"Date of change from the former name, if any.\",\n",
    "            \"afs\": \"Filer status with the SEC at the time of submission:\\n\\\n",
    "1-LAF=Large Accelerated,\\n\\\n",
    "2-ACC=Accelerated,\\n\\\n",
    "3-SRA=Smaller Reporting Accelerated,\\n\\\n",
    "4-NON=Non-Accelerated,\\n\\\n",
    "5-SML=Smaller Reporting Filer,\\n\\\n",
    "NULL=not assigned.\",\n",
    "            \"wksi\": \"Well Known Seasoned Issuer (WKSI). An issuer that meets specific SEC requirements at some point during a 60-day period preceding the date the issuer satisfies its obligation to update its shelf registration statement.\",\n",
    "            \"fye\": \"Fiscal Year End Date, rounded to nearest month-end.\",\n",
    "            \"form\": \"The submission type of the registrant’s filing.\",\n",
    "            \"period\": \"Balance Sheet Date, rounded to nearest month-end.\",\n",
    "            \"fy\": \"Fiscal Year Focus (as defined in EFM Ch. 6).\",\n",
    "            \"fp\": \"Fiscal Period Focus (as defined in EFM Ch. 6) within Fiscal Year. The 10-Q for the 1st, 2nd and 3rd quarters would have a fiscal period focus of Q1, Q2 (or H1), and Q3 (or M9) respectively, and a 10-K would have a fiscal period focus of FY.\",\n",
    "            \"filed\": \"The date of the registrant’s filing with the Commission.\",\n",
    "            \"accepted\": \"The acceptance date and time of the registrant’s filing with the Commission. Filings accepted after 5:30pm EST are considered filed on the following business day.\",\n",
    "            \"prevrpt\": \"Previous Report –TRUE indicates that the submission information was subsequently amended.\",\n",
    "            \"detail\": \"TRUE indicates that the XBRL submission contains quantitative disclosures within the footnotes and schedules at the required detail level (e.g., each amount).\",\n",
    "            \"instance\": \"The name of the submitted XBRL Instance Document (EX-101.INS) type data file. The name often begins with the company ticker symbol.\",\n",
    "            \"nciks\": \"Number of Central Index Keys (CIK) of registrants (i.e., business units) included in the consolidating entity’s submitted filing.\",\n",
    "            \"aciks\": \"Additional CIKs of co-registrants included in  a consolidating entity’s EDGAR submission, separated by spaces. If there are no other co-registrants (i.e., nciks=1), the value of aciks is NULL.  For a very small number of filers, the list of co-registrants is too long to fit in the field.  Where this is the case, PARTIAL will appear at the end of the list indicating that not all co-registrants’ CIKs are included in the field; users should refer to the complete submission file for all CIK information.\",\n",
    "        }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ea32d-d2b4-48a8-b1c8-529efc30429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_meta_content = {\n",
    "    \"dataset_key\": \"SEC-DERA\",\n",
    "    \"tablename\": \"num\",\n",
    "    \"title\": \"SEC DERA Disclosures: NUM\",\n",
    "    \"description\": \"\"\"The DERA Financial Statement Data Sets provide numeric information from the face financials of all financial statements.\n",
    "    \n",
    "    This data is extracted from exhibits to corporate financial reports filed with the Commission using eXtensible Business Reporting Language (XBRL).  As compared to the more extensive Financial Statement and Notes Data Sets, which provide the numeric and narrative disclosures from all financial statements and their notes, the Financial Statement Data Sets are more compact.\"\"\",\n",
    "    \"version\": \"quarterly\",\n",
    "    \"release_date\": \"rolling\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"adsh\": \"Accession Number. The 20-character string formed from the 18-digit number assigned by the SEC to each EDGAR submission.\",\n",
    "            \"tag\": \"The unique identifier (name) for a tag in a specific taxonomy release.\",\n",
    "            \"version\": \"For a standard tag, an identifier for the taxonomy; otherwise the accession number where the tag was defined.\",\n",
    "            \"coreg\": \"If specified, indicates a specific co-registrant, the parent company, or other entity (e.g., guarantor). NULL indicates the consolidated entity.\",\n",
    "            \"ddate\": \"The end date for the data value, rounded to the nearest month end.\",\n",
    "            \"qtrs\": \"The count of the number of quarters represented by the data value, rounded to the nearest whole number. “0” indicates it is a point-in-time value.\",\n",
    "            \"uom\": \"The unit of measure for the value.\",\n",
    "            \"value\": \"The value. This is not scaled, it is as found in the Interactive Data file, but is limited to four digits to the right of the decimal point.\",\n",
    "            \"footnote\": \"The text of any superscripted footnotes on the value, as shown on the statement page, truncated to 512 characters, or if there is no footnote, then this field will be blank.\",\n",
    "        }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a6a0d5-9a51-4619-9b8b-8c8dc32807ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_meta_content = {\n",
    "    \"dataset_key\": \"SEC-DERA\",\n",
    "    \"tablename\": \"tag\",\n",
    "    \"title\": \"SEC DERA Disclosures: TAG\",\n",
    "    \"description\": \"\"\"The DERA Financial Statement Data Sets provide numeric information from the face financials of all financial statements.\n",
    "    \n",
    "    This data is extracted from exhibits to corporate financial reports filed with the Commission using eXtensible Business Reporting Language (XBRL).  As compared to the more extensive Financial Statement and Notes Data Sets, which provide the numeric and narrative disclosures from all financial statements and their notes, the Financial Statement Data Sets are more compact.\"\"\",\n",
    "    \"version\": \"quarterly\",\n",
    "    \"release_date\": \"rolling\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"tag\": \"The unique identifier (name) for a tag in a specific taxonomy release.\",\n",
    "            \"version\": \"For a standard tag, an identifier for the taxonomy; otherwise the accession number where the tag was defined.\",\n",
    "            \"custom\": \"1 if tag is custom (version=adsh), 0 if it is standard. Note: This flag is technically redundant with the version and adsh columns.\",\n",
    "            \"abstract\": \"1 if the tag is not used to represent a numeric fact.\",\n",
    "            \"datatype\": \"If abstract=1, then NULL, otherwise the data type (e.g., monetary) for the tag.\",\n",
    "            \"iord\": \"If abstract=1, then NULL; otherwise, “I” if the value is a point-in time, or “D” if the value is a duration.\",\n",
    "            \"crdr\": \"If datatype = monetary, then the tag’s natural accounting balance (debit or credit); if not defined, then NULL.\",\n",
    "            \"tlabel\": \"If a standard tag, then the label text provided by the taxonomy, otherwise the text provided by the filer. A tag which had neither would have a NULL value here.\",\n",
    "            \"doc\": \"The detailed definition for the tag (truncated to 2048 characters). If a standard tag, then the text provided by the taxonomy, otherwise the text assigned by the filer. Some tags have neither, and this field is NULL.\",\n",
    "        }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa87f18-3b60-49d6-9374-ef29e9a6fd4d",
   "metadata": {},
   "source": [
    "Convert custom metadata content in json format into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267feab1-5c19-43b2-8c04-180d77b33c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_check = engine.execute(f\"create schema if not exists {ingest_catalog}.{meta_schema_name}\")\n",
    "for row in schema_check.fetchall():\n",
    "    print(row)\n",
    "\n",
    "table_check = engine.execute(f\"drop table if exists {ingest_catalog}.{meta_schema_name}.{meta_table_name_dataset}\")\n",
    "for row in table_check.fetchall():\n",
    "    print(row)\n",
    "\n",
    "for meta_content in [sub_meta_content, num_meta_content, tag_meta_content]:\n",
    "    df_meta_table = pd.json_normalize(meta_content, max_level=0)\n",
    "    df_meta_table = df_meta_table.drop(\"fields\", axis=1)\n",
    "    df_meta_table[\"schema\"] = ingest_schema\n",
    "    df_meta_table = df_meta_table.convert_dtypes()\n",
    "    display(df_meta_table)\n",
    "    df_meta_table.info(verbose=True)\n",
    "\n",
    "    if meta_content == sub_meta_content:\n",
    "        # We only need to create the metadata table once\n",
    "        schema_meta_table = create_table_schema_pairs(df_meta_table)\n",
    "        tabledef = f\"\"\"\n",
    "create table if not exists {ingest_catalog}.{meta_schema_name}.{meta_table_name_dataset}(\n",
    "{schema_meta_table}\n",
    ") with (\n",
    "    format = 'ORC',\n",
    "    partitioning = array['dataset_key']\n",
    ")\n",
    "\"\"\"\n",
    "        print(tabledef)\n",
    "        qres = engine.execute(tabledef)\n",
    "        print(qres.fetchall())\n",
    "\n",
    "        meta_table_create = engine.execute(tabledef)\n",
    "        for row in meta_table_create.fetchall():\n",
    "            print(row)\n",
    "\n",
    "    df_meta_table.to_sql(\n",
    "        meta_table_name_dataset, con=engine, schema=meta_schema_name, if_exists=\"append\", index=False, method=\"multi\"\n",
    "    )\n",
    "\n",
    "    list_values_meta_table = df_meta_table.values.tolist()\n",
    "    display(list_values_meta_table[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0c4ea-89c6-4ab7-895c-89217f8c962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_query_table = f\"SELECT * FROM {ingest_catalog}.{meta_schema_name}.{meta_table_name_dataset} limit 10\"\n",
    "print(meta_query_table)\n",
    "meta_table_query = engine.execute(meta_query_table)\n",
    "for row in meta_table_query.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac0007-d564-46e8-a111-a568b05355de",
   "metadata": {},
   "source": [
    "Create metadata table for fields information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1964ef-40f5-4285-b65d-c02ba2aa053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_check = engine.execute(f\"drop table if exists {ingest_catalog}.{meta_schema_name}.{meta_table_name_fields}\")\n",
    "for row in table_check.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1742e-4572-4b06-9921-2b95f17cd175",
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta_content in [sub_meta_content, num_meta_content, tag_meta_content]:\n",
    "    df = pd.json_normalize(meta_content, record_path=[\"fields\"]).T\n",
    "    df[\"tablename\"] = meta_content[\"tablename\"]\n",
    "    df[\"dataset_key\"] = meta_content[\"dataset_key\"]\n",
    "    df = df.reset_index().convert_dtypes()\n",
    "    df.rename(columns={\"index\": \"fieldname\", 0: \"description\"}, inplace=True)\n",
    "    df_meta_fields = df\n",
    "    display(df_meta_fields)\n",
    "    df_meta_fields.info(verbose=True)\n",
    "\n",
    "    if meta_content == sub_meta_content:\n",
    "        # Only need to create this table once\n",
    "        schema_meta_fields = create_table_schema_pairs(df_meta_fields)\n",
    "        tabledef = f\"\"\"\n",
    "create table if not exists {ingest_catalog}.{meta_schema_name}.{meta_table_name_fields}(\n",
    "{schema_meta_fields}\n",
    ") with (\n",
    "    format = 'ORC',\n",
    "    partitioning = array['dataset_key']\n",
    ")\n",
    "\"\"\"\n",
    "        print(tabledef)\n",
    "        qres = engine.execute(tabledef)\n",
    "        print(qres.fetchall())\n",
    "\n",
    "        meta_fields_create = engine.execute(tabledef)\n",
    "        for row in meta_fields_create.fetchall():\n",
    "            print(row)\n",
    "\n",
    "    df_meta_fields.to_sql(\n",
    "        meta_table_name_fields, con=engine, schema=meta_schema_name, if_exists=\"append\", index=False, method=\"multi\"\n",
    "    )\n",
    "\n",
    "    list_values_meta_fields = df_meta_fields.values.tolist()\n",
    "    display(list_values_meta_fields[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff38efd-bcdc-4bb1-b40c-5928a8f494e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_query_fields = f\"SELECT * FROM {ingest_catalog}.{meta_schema_name}.{meta_table_name_fields} limit 10\"\n",
    "print(meta_query_table)\n",
    "meta_table_query = engine.execute(meta_query_table)\n",
    "for row in meta_table_query.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec33535-156e-4697-bfbf-230de38769cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
